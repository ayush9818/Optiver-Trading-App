{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4138fd9e-113a-4c52-bea6-b7ad1bd0192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use get stock data and store into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66ccf51-052e-4cf3-b83e-75146186e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('../optiver_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9d59a-7a39-4787-a587-3dc023073bcc",
   "metadata": {},
   "source": [
    "## Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900b6bfe-a59b-4f1d-9996-39d1a7425a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    features = ['seconds_in_bucket', 'imbalance_buy_sell_flag', 'imbalance_size', 'matched_size',\n",
    "                'bid_size', 'ask_size', 'reference_price', 'far_price', 'near_price', 'ask_price',\n",
    "                'bid_price', 'wap', 'imb_s1', 'imb_s2']\n",
    "    \n",
    "    # Create a copy to modify\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Imbalance features\n",
    "    df['imb_s1'] = (df['bid_size'] - df['ask_size']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['imb_s2'] = (df['imbalance_size'] - df['matched_size']) / (df['matched_size'] + df['imbalance_size'])\n",
    "    \n",
    "    # Price difference features\n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    for i, a in enumerate(prices):\n",
    "        for j, b in enumerate(prices[i+1:], i+1):\n",
    "            df[f'{a}_{b}_diff'] = df[a] - df[b]\n",
    "            features.append(f'{a}_{b}_diff')\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971e8ba6-5159-4492-964f-0d066d819ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_initial_model(df_train, start_date_id, end_date_id):\n",
    "    # Filter to include only the days within the specified range\n",
    "    df_train = df_train[(df_train['date_id'] >= start_date_id) & (df_train['date_id'] <= end_date_id)].copy()\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_names = df_train.drop(columns=['target', 'date_id', 'stock_id']).columns.tolist()\n",
    "    X = df_train[feature_names].values\n",
    "    y = df_train['target'].values\n",
    "\n",
    "    # Time series cross-validation\n",
    "    xgboost_models = []\n",
    "    xgboost_cv_errors = []\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "        y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
    "\n",
    "        # XGBoost model\n",
    "        dtrain = xgb.DMatrix(X_train_cv, label=y_train_cv)\n",
    "        dtest = xgb.DMatrix(X_test_cv, label=y_test_cv)\n",
    "        params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'learning_rate': 0.01}\n",
    "        model_xgb = xgb.train(params, dtrain, num_boost_round=50, evals=[(dtest, 'eval')],\n",
    "                              early_stopping_rounds=30, verbose_eval=False)\n",
    "        xgboost_models.append(model_xgb)\n",
    "        xgboost_cv_errors.append(model_xgb.best_score)\n",
    "\n",
    "    average_mae = np.mean(xgboost_cv_errors)\n",
    "    print(f\"Average MAE across all folds: {average_mae}\")\n",
    "\n",
    "    # Save the final model\n",
    "    model_filename = f'XGBoost_{start_date_id}_{end_date_id}.pickle'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(xgboost_models[-1], f)\n",
    "\n",
    "    return xgboost_models[-1], model_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d5831d-b969-42b7-a188-5b7c16babe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE across all folds: 6.650049333042716\n",
      "Initial model trained and saved as: XGBoost_50_400.pickle\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering + model parameters \n",
    "df_train = df.dropna(subset=['target']).copy()\n",
    "df_train, feature_names = generate_features(df_train)\n",
    "start_date = 50\n",
    "end_date = 400\n",
    "\n",
    "initial_model, model_filename = train_initial_model(df_train, start_date, end_date)\n",
    "print(\"Initial model trained and saved as:\", model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93c100-fe5c-41d2-a41b-ac3a0477cdbf",
   "metadata": {},
   "source": [
    "## Continual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813f7356-f0ed-4390-a027-b333d7d0e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_training(df_train, date_ids, initial_model, retrain_day):\n",
    "    # Filter data up to the retrain_day\n",
    "    df_train_filtered = df_train[df_train['date_id'] <= retrain_day].copy()\n",
    "\n",
    "    # Prepare features and target\n",
    "    feature_names = df_train_filtered.drop(columns=['target', 'date_id', 'stock_id']).columns.tolist()\n",
    "    X_train = df_train_filtered[feature_names].values\n",
    "    y_train = df_train_filtered['target'].values\n",
    "\n",
    "    # Retrain the model with cumulative data\n",
    "    xgb.set_config(verbosity=0)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'learning_rate': 0.01}\n",
    "    model = xgb.train(params, dtrain, num_boost_round=50, verbose_eval=False, xgb_model=initial_model)\n",
    "\n",
    "    # Save the final model\n",
    "    model_filename = f'XGBoost_retrain_{retrain_day}.pickle'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    return model, model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cf2ac92-abea-4e22-801d-fcefe274e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained and saved as: XGBoost_retrain_401.pickle\n"
     ]
    }
   ],
   "source": [
    "retrain_day = 401\n",
    "final_model, model_filename = incremental_training(df_train, date_ids, initial_model, retrain_day)\n",
    "print(\"Final model trained and saved as:\", model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb561991-520c-48eb-ac80-eac9f003c5a0",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438e14d1-2117-401c-8dc1-2a1509a396a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results saved as: inference_401.csv\n"
     ]
    }
   ],
   "source": [
    "def inference_next_day(df_train, model_file):\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    last_day = int(model_file.split('_')[-1].split('.')[0])\n",
    "    inference_day = last_day + 1\n",
    "\n",
    "    # Filter data for inference day\n",
    "    df_inference = df_train[df_train['date_id'] == inference_day].copy()\n",
    "\n",
    "    # Prepare features for inference\n",
    "    feature_names = df_inference.drop(columns=['target', 'date_id', 'stock_id']).columns.tolist()\n",
    "    X_inference = df_inference[feature_names].values\n",
    "\n",
    "    # Perform inference\n",
    "    d_inference = xgb.DMatrix(X_inference)\n",
    "    predictions = model.predict(d_inference)\n",
    "\n",
    "    # Add predictions to the DataFrame\n",
    "    df_inference['prediction'] = predictions\n",
    "\n",
    "    # Save the results\n",
    "    inference_filename = f'inference_{inference_day}.csv'\n",
    "    df_inference.to_csv(inference_filename, index=False)\n",
    "\n",
    "    return inference_filename\n",
    "\n",
    "\n",
    "inference_file = inference_next_day(df_train, 'XGBoost_50_400.pickle')\n",
    "print(\"Inference results saved as:\", inference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd04ba1-f14d-4f31-869a-b7cca8a6bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
